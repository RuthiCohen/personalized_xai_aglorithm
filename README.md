# Personalized XAI Algorithm â€“ Experiments Overview

This repository contains two experiments focused on generating data and applying explainability techniques using neural networks and symbolic reasoning datasets.

---

## ğŸ§ª Experiment 1: EMNIST Alphabet with CNN

This experiment involves generating data using a Convolutional Neural Network (CNN) trained on the EMNIST alphabet dataset.

### ğŸ“¥ Dataset
To run the experiment, download the following files from the [EMNIST Kaggle page](https://www.kaggle.com/datasets/crawford/emnist):

- `emnist-letters-train.csv`
- `emnist-letters-test.csv`

Place both files inside the `experiment 1/` directory.

### ğŸ“‚ Important File
- `eminst.ipynb` â€“ Jupyter notebook for creating training data and generating explanations.

### ğŸ“Š Output
- After running the notebook, a model file `emnist_model.keras` will be generated with a final accuracy of **83.23%**.
  (the trained model is in the "trained_models/emnist_model.keras")
- The notebook includes the implementation of **two types of explanations** for model predictions.

### ğŸ—‚ï¸ Explanations Output Folder Structure

Inside the `experiment_1/` folder, you'll find a directory called `experiment_images`, which contains:

- **`pre_task/`** â€“ Contains a comprehensive image showcasing examples of all the letters used in the experiment.
- **`prediction_task_data.json`** â€“ A JSON file storing the predicted and actual values for each letter in the prediction task.
- **`prediction_task/`**
  - **`emnist/`**
    - **`9_errors/`** â€“ Contains examples where the model made incorrect predictions.
    - **`15_correct/`** â€“ Contains examples where the model predicted correctly.

Each of the above (`9_errors` and `15_correct`) contains:

- A `letters/` folder with the relevant letter images.
- A `llm/` folder with explanations generated by a language model (LLM).
- A `shap/` folder with visual explanations using SHAP values.
- An `example_based/` folder containing explanations based on a 3-nearest neighbors (3-NN) approach.

---

## ğŸ§  Experiment 2: Raven-10k with DCNet

This experiment involves creating reasoning data using the **DCNet** model, adapted from the original repository:  
ğŸ”— https://github.com/visiontao/dcnet/tree/main

### ğŸ“¥ Dataset
The model is trained and evaluated on the **RAVEN-10k** dataset, which can be downloaded from:  
ğŸ”— https://wellyzhang.github.io/project/raven.html#dataset  
> Click on the **Google Drive link** to download the dataset and place it in the appropriate directory.
> Place the RAVEN-10k in a directory: `experiment_2/dataset/RAVEN-10000`

### ğŸ“‚ Important Files
- `generate_data.py` â€“ Creates a folder containing visual reasoning puzzles and runs different explanation methods on them.
- `main.py` â€“ Demonstrates example explanations for a selected Raven matrix.
- `train.py` â€“ Trains the DCNet model across multiple folds.(DCnet repository)
- `test.py` â€“ Test the DCNet models and returns the accuracy results. (DCnet repository)

### ğŸ“Š Output
- Training will result in **30 saved models**.
- The selected model used for explanations in this repository is `model_02.pth`, which achieved an accuracy of **87.914%**.

### ğŸ—‚ï¸ Explanations Output Folder Structure

Inside the `experiment_2/experiment_images/` directory, youâ€™ll find:

- `LLM/`: Contains images shown in the experiment with **LLM-generated explanations**.
  - `correct/`: Examples where the LLM gave the correct answer.
  - `wrong/`: Examples where the LLM gave the wrong answer.
  - Inside each, folders are grouped by **RAVEN dataset category** (e.g., `center_single`) and then by question ID (e.g., `8558`).

- `occlusion_sensitivity/`: Same structure as `LLM/`, but with **DCNet predictions and occlusion-based explanations**.
- `llm explanations vs.xlsx`: A spreadsheet comparing actual vs. predicted answers from both the LLM and the occlusion sensitivity method.
---

Feel free to open an issue or contribute if you're interested in extending the experiments or testing alternative explainability methods.
