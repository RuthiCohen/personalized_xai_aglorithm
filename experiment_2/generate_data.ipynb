{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3698de31-60e6-46da-a69b-9692f2072290",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "from dcnet import DCNet\n",
    "from dataset_utility import ToTensor\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bfa830af-86ab-4447-a0e8-624a44530e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {\n",
    "    \"distribute_four\": {\n",
    "        \"correct\": [9138, 3148], \n",
    "        \"wrong\": [929]\n",
    "    },\n",
    "    \"distribute_nine\": {\n",
    "        \"correct\": [4088, 3149],\n",
    "        \"wrong\": [9138, 1228]\n",
    "    },\n",
    "    \"center_single\": {\n",
    "        \"correct\": [8558, 2529],\n",
    "        \"wrong\": [8608]\n",
    "    },\n",
    "    \"in_center_single_out_center_single\": {\n",
    "        \"correct\": [9138, 5878],\n",
    "        \"wrong\": [7409, 7899]\n",
    "    },\n",
    "    \"in_distribute_four_out_center_single\": {\n",
    "        \"correct\": [5878, 7789, 219],\n",
    "        \"wrong\": [2469]\n",
    "    }, \n",
    "    \"left_center_single_right_center_single\": {\n",
    "        \"correct\": [5348, 218],\n",
    "        \"wrong\": [7969]\n",
    "    },\n",
    "    \"up_center_single_down_center_single\": {\n",
    "        \"correct\": [7029, 219],\n",
    "        \"wrong\": [8669]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9281d0da-0ba8-439d-a555-c1fb8ce333d2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DCNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (res1): ResBlock(\n",
       "    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (downsample): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (choice_contrast1): Sequential(\n",
       "    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (res2): ResBlock(\n",
       "    (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (downsample): Sequential(\n",
       "      (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (choice_contrast2): Sequential(\n",
       "    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (res3): ResBlock(\n",
       "    (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (downsample): Sequential(\n",
       "      (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (mlp): MLP(\n",
       "    (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (relu1): ReLU()\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model\n",
    "device = \"mps\"\n",
    "model = DCNet().to(torch.device(device))\n",
    "model.load_state_dict(torch.load('model_02.pth', map_location=torch.device(\"mps\")))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3afc1f-339c-435f-8b4f-044187c5f68b",
   "metadata": {},
   "source": [
    "# Find correct/wrong prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d26712c-718b-4a48-ad52-0e80a035d231",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def find_predictions_by_match(model, base_dir, match_op='==', max_files=20, img_size=96):\n",
    "    \"\"\"\n",
    "    Find files where the model prediction matches or mismatches the label based on `match_op`.\n",
    "\n",
    "    Args:\n",
    "        model: Trained DCNet model\n",
    "        base_dir: Path to RAVEN directory\n",
    "        match_op: '==' for correct, '!=' for incorrect\n",
    "        max_files: Max number of files to return\n",
    "        img_size: Resize shape (img_size, img_size)\n",
    "\n",
    "    Returns:\n",
    "        List of matching file paths\n",
    "    \"\"\"\n",
    "    assert match_op in ['==', '!='], \"match_op must be '==' or '!='\"\n",
    "\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    matching_files = []\n",
    "    pattern = re.compile(r'RAVEN_\\d+_test\\.npz')\n",
    "\n",
    "    for root, _, files in os.walk(base_dir):\n",
    "        for fname in files:\n",
    "            if not pattern.match(fname):\n",
    "                continue\n",
    "\n",
    "            path = os.path.join(root, fname)\n",
    "            data = np.load(path)\n",
    "            images = data['image']\n",
    "            target = int(data['target'])\n",
    "\n",
    "            # Resize and convert to tensor\n",
    "            images_resized = np.stack([\n",
    "                cv2.resize(img, (img_size, img_size), interpolation=cv2.INTER_NEAREST)\n",
    "                for img in images\n",
    "            ])\n",
    "            images_tensor = torch.tensor(images_resized, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = model(images_tensor)\n",
    "                pred = torch.argmax(output, dim=1).item()\n",
    "\n",
    "            # Check condition\n",
    "            is_match = pred == target\n",
    "            if (match_op == '==' and is_match) or (match_op == '!=' and not is_match):\n",
    "                matching_files.append(path)\n",
    "                if len(matching_files) == max_files:\n",
    "                    return matching_files\n",
    "\n",
    "    return matching_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94423641-5a06-4274-9957-6359a34b41a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"dataset/RAVEN-10000/distribute_four\"\n",
    "# Find first 20 correct predictions\n",
    "correct = find_predictions_by_match(model, data_path, match_op='==')\n",
    "\n",
    "# Find first 20 incorrect predictions\n",
    "incorrect = find_predictions_by_match(model, data_path, match_op='!=')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53d60112-1706-4f17-b14a-44ae4f5bc841",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dataset/RAVEN-10000/distribute_four/RAVEN_1228_test.npz',\n",
       " 'dataset/RAVEN-10000/distribute_four/RAVEN_4089_test.npz',\n",
       " 'dataset/RAVEN-10000/distribute_four/RAVEN_2529_test.npz',\n",
       " 'dataset/RAVEN-10000/distribute_four/RAVEN_5879_test.npz',\n",
       " 'dataset/RAVEN-10000/distribute_four/RAVEN_7788_test.npz',\n",
       " 'dataset/RAVEN-10000/distribute_four/RAVEN_2289_test.npz',\n",
       " 'dataset/RAVEN-10000/distribute_four/RAVEN_7029_test.npz',\n",
       " 'dataset/RAVEN-10000/distribute_four/RAVEN_9698_test.npz',\n",
       " 'dataset/RAVEN-10000/distribute_four/RAVEN_4728_test.npz',\n",
       " 'dataset/RAVEN-10000/distribute_four/RAVEN_1918_test.npz',\n",
       " 'dataset/RAVEN-10000/distribute_four/RAVEN_1919_test.npz',\n",
       " 'dataset/RAVEN-10000/distribute_four/RAVEN_929_test.npz',\n",
       " 'dataset/RAVEN-10000/distribute_four/RAVEN_869_test.npz',\n",
       " 'dataset/RAVEN-10000/distribute_four/RAVEN_868_test.npz',\n",
       " 'dataset/RAVEN-10000/distribute_four/RAVEN_4669_test.npz',\n",
       " 'dataset/RAVEN-10000/distribute_four/RAVEN_1859_test.npz',\n",
       " 'dataset/RAVEN-10000/distribute_four/RAVEN_359_test.npz',\n",
       " 'dataset/RAVEN-10000/distribute_four/RAVEN_358_test.npz',\n",
       " 'dataset/RAVEN-10000/distribute_four/RAVEN_2469_test.npz',\n",
       " 'dataset/RAVEN-10000/distribute_four/RAVEN_1369_test.npz']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# correct\n",
    "incorrect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664416f2-ff42-44d7-b798-7a5295547f00",
   "metadata": {},
   "source": [
    "# Save RAVEN questions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f72124a-42b2-4c6e-a12d-5ef8c203554e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_raven_question_visual(path, save_dir=\"raven_visuals\"):\n",
    "    \"\"\"\n",
    "    Load a RAVEN .npz file and save two plots:\n",
    "    1. The 3x3 matrix with the missing panel replaced by '?'\n",
    "    2. The 8 answer choices with labels\n",
    "\n",
    "    Args:\n",
    "        path (str): Path to a .npz file (e.g., RAVEN_x_test.npz)\n",
    "        save_dir (str): Directory to save output images\n",
    "\n",
    "    Returns:\n",
    "        (str, str): Paths to the saved matrix and choices images\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    data = np.load(path)\n",
    "    images = data['image']  # shape: (16, H, W)\n",
    "    base_filename = os.path.splitext(os.path.basename(path))[0]\n",
    "\n",
    "    # --- Save Matrix with \"?\" ---\n",
    "    fig, axs = plt.subplots(3, 3, figsize=(6, 6))\n",
    "    for i in range(8):\n",
    "        ax = axs[i // 3, i % 3]\n",
    "        ax.imshow(images[i], cmap='gray')\n",
    "        ax.set_xticks([]); ax.set_yticks([])\n",
    "        ax.set_xticks([0, images[i].shape[1]], minor=True)\n",
    "        ax.set_yticks([0, images[i].shape[0]], minor=True)\n",
    "        ax.grid(which='minor', color='black', linewidth=1.5)\n",
    "\n",
    "    ax = axs[2, 2]\n",
    "    ax.axis('off')\n",
    "    ax.text(0.5, 0.5, '?', transform=ax.transAxes, fontsize=40, ha='center', va='center')\n",
    "\n",
    "    for row in range(3):\n",
    "        axs[row][0].set_ylabel(f\"Row {row+1}\", fontsize=12)\n",
    "    for col in range(3):\n",
    "        axs[0][col].set_title(f\"Col {col+1}\", fontsize=12)\n",
    "\n",
    "    plt.suptitle('Matrix (Choose the missing piece)', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    matrix_path = os.path.join(save_dir, base_filename + \"_matrix.png\")\n",
    "    plt.savefig(matrix_path, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # --- Save Answer Choices ---\n",
    "    fig, axs = plt.subplots(1, 8, figsize=(16, 2.5))\n",
    "    for i in range(8):\n",
    "        axs[i].imshow(images[8 + i], cmap='gray')\n",
    "        axs[i].set_xticks([]); axs[i].set_yticks([])\n",
    "        axs[i].set_xticks([0, images[i].shape[1]], minor=True)\n",
    "        axs[i].set_yticks([0, images[i].shape[0]], minor=True)\n",
    "        axs[i].grid(which='minor', color='black', linewidth=1.0)\n",
    "        axs[i].set_title(f\"Choice {i}\", fontsize=10)\n",
    "\n",
    "    plt.suptitle('Answer Choices (0 to 7)', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    choices_path = os.path.join(save_dir, base_filename + \"_choices.png\")\n",
    "    plt.savefig(choices_path, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    return matrix_path, choices_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d27d5c37-139c-4a27-a20c-16ea817c55ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"dataset/RAVEN-10000/\"\n",
    "save_root = \"experiment/\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76f54ac4-cb5a-43ee-8b25-9eb8b9f54a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "for category, results in data_dict.items():\n",
    "    for label in [\"correct\", \"wrong\"]:\n",
    "        for sample_id in results[label]:\n",
    "            fname = f\"RAVEN_{sample_id}_test.npz\"\n",
    "            path = os.path.join(base_dir, category, fname)\n",
    "            out_dir = os.path.join(save_root, label, category, str(sample_id))\n",
    "            \n",
    "            try:\n",
    "                matrix_path, choices_path = save_raven_question_visual(path, save_dir=out_dir)\n",
    "                # print(f\"Saved: {matrix_path}, {choices_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error with {path}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c2e798-41d9-4f7e-919c-dd908667df9d",
   "metadata": {},
   "source": [
    "# Occlusion Sensitivity Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7530e6f4-6b3e-49dc-a23b-06fd199cd5c2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def visualize_occlusion_with_heatmaps(model, images_tensor, original_images, pred, save_dir=\"outputs\", filename_prefix=\"heatmap\"):\n",
    "    \"\"\"\n",
    "    Generates and saves occlusion heatmap visualizations for a Raven matrix.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained DCNet model\n",
    "        images_tensor: Tensor of shape (1, 16, H, W)\n",
    "        original_images: Numpy array of shape (16, H, W)\n",
    "        pred: int, predicted answer index (0–7)\n",
    "        save_dir: folder to save the image(s)\n",
    "        filename_prefix: prefix for saved filenames\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    def get_occlusion_sensitivity(model, images_tensor, window_size=20, stride=10):\n",
    "        with torch.no_grad():\n",
    "            original_output = model(images_tensor)\n",
    "            pred_idx = torch.argmax(original_output, dim=1).item()\n",
    "            original_score = original_output[0, pred_idx].item()\n",
    "\n",
    "        sensitivity_maps = []\n",
    "        image_size = images_tensor.shape[-1]\n",
    "\n",
    "        for img_idx in range(16):\n",
    "            sensitivity_map = np.zeros((image_size, image_size))\n",
    "            for i in range(0, image_size - window_size + 1, stride):\n",
    "                for j in range(0, image_size - window_size + 1, stride):\n",
    "                    modified_input = images_tensor.clone().detach()\n",
    "                    modified_input[0, img_idx, i:i+window_size, j:j+window_size] = 0\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        output = model(modified_input)\n",
    "                        new_score = output[0, pred_idx].item()\n",
    "\n",
    "                    score_change = abs(original_score - new_score)\n",
    "                    sensitivity_map[i:i+window_size, j:j+window_size] += score_change\n",
    "\n",
    "            # Normalize\n",
    "            map_max = sensitivity_map.max()\n",
    "            map_min = sensitivity_map.min()\n",
    "            if map_max == map_min:\n",
    "                sensitivity_map = np.ones_like(sensitivity_map) * 0.5\n",
    "            else:\n",
    "                sensitivity_map = (sensitivity_map - map_min) / (map_max - map_min + 1e-8)\n",
    "\n",
    "            sensitivity_maps.append(sensitivity_map)\n",
    "\n",
    "        return sensitivity_maps\n",
    "\n",
    "    # ---- Generate sensitivity maps\n",
    "    sensitivity_maps = get_occlusion_sensitivity(model, images_tensor)\n",
    "\n",
    "    # ---- Plot 3×3 matrix with heatmaps + colorbar\n",
    "    fig, axs = plt.subplots(3, 3, figsize=(7, 7))\n",
    "    for i in range(8):\n",
    "        ax = axs[i // 3, i % 3]\n",
    "        heat = ax.imshow(original_images[i], cmap='gray', alpha=0.5)\n",
    "        heatmap = ax.imshow(sensitivity_maps[i], cmap='jet', alpha=0.5)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    \n",
    "    # Add the missing panel\n",
    "    ax = axs[2, 2]\n",
    "    ax.axis('off')\n",
    "    ax.text(0.5, 0.5, '?', transform=ax.transAxes, fontsize=40, ha='center', va='center')\n",
    "    \n",
    "    # Add colorbar to the figure\n",
    "    cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])  # [left, bottom, width, height]\n",
    "    fig.colorbar(heatmap, cax=cbar_ax)\n",
    "    \n",
    "    plt.suptitle('Matrix with Occlusion Sensitivity', fontsize=14)\n",
    "    plt.tight_layout(rect=[0, 0, 0.9, 1])  # Make space for colorbar\n",
    "    matrix_path = os.path.join(save_dir, f\"{filename_prefix}_matrix.png\")\n",
    "    plt.savefig(matrix_path)\n",
    "    plt.close()\n",
    "\n",
    "    # ---- Plot choices with heatmaps\n",
    "    fig, axs = plt.subplots(1, 8, figsize=(16, 2.5))\n",
    "    for i in range(8):\n",
    "        ax = axs[i]\n",
    "        ax.imshow(original_images[8 + i], cmap='gray', alpha=0.5)\n",
    "        ax.imshow(sensitivity_maps[8 + i], cmap='jet', alpha=0.5)\n",
    "        ax.axis('off')\n",
    "        ax.set_title(f\"{i}\", fontsize=10)\n",
    "\n",
    "    plt.suptitle(f'Answer Choices (Predicted: {pred})', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    choices_path = os.path.join(save_dir, f\"{filename_prefix}_choices.png\")\n",
    "    plt.savefig(choices_path)\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Saved matrix heatmap to: {matrix_path}\")\n",
    "    print(f\"Saved choices heatmap to: {choices_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "335b4e41-fc4e-4989-8fd5-b9bda38553d1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pn/yynvkvmn45x9jcrwyvqv879r0000gn/T/ipykernel_13370/3409464297.py:72: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  plt.tight_layout(rect=[0, 0, 0.9, 1])  # Make space for colorbar\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved matrix heatmap to: ruti/correct/distribute_four/9138/occulstion_sensitivity/RAVEN_9138_test.npz_matrix.png\n",
      "Saved choices heatmap to: ruti/correct/distribute_four/9138/occulstion_sensitivity/RAVEN_9138_test.npz_choices.png\n",
      "Saved matrix heatmap to: ruti/correct/distribute_four/3148/occulstion_sensitivity/RAVEN_3148_test.npz_matrix.png\n",
      "Saved choices heatmap to: ruti/correct/distribute_four/3148/occulstion_sensitivity/RAVEN_3148_test.npz_choices.png\n",
      "Saved matrix heatmap to: ruti/wrong/distribute_four/929/occulstion_sensitivity/RAVEN_929_test.npz_matrix.png\n",
      "Saved choices heatmap to: ruti/wrong/distribute_four/929/occulstion_sensitivity/RAVEN_929_test.npz_choices.png\n",
      "Saved matrix heatmap to: ruti/correct/distribute_nine/4088/occulstion_sensitivity/RAVEN_4088_test.npz_matrix.png\n",
      "Saved choices heatmap to: ruti/correct/distribute_nine/4088/occulstion_sensitivity/RAVEN_4088_test.npz_choices.png\n",
      "Saved matrix heatmap to: ruti/correct/distribute_nine/3149/occulstion_sensitivity/RAVEN_3149_test.npz_matrix.png\n",
      "Saved choices heatmap to: ruti/correct/distribute_nine/3149/occulstion_sensitivity/RAVEN_3149_test.npz_choices.png\n",
      "Saved matrix heatmap to: ruti/wrong/distribute_nine/9138/occulstion_sensitivity/RAVEN_9138_test.npz_matrix.png\n",
      "Saved choices heatmap to: ruti/wrong/distribute_nine/9138/occulstion_sensitivity/RAVEN_9138_test.npz_choices.png\n",
      "Saved matrix heatmap to: ruti/wrong/distribute_nine/1228/occulstion_sensitivity/RAVEN_1228_test.npz_matrix.png\n",
      "Saved choices heatmap to: ruti/wrong/distribute_nine/1228/occulstion_sensitivity/RAVEN_1228_test.npz_choices.png\n",
      "Saved matrix heatmap to: ruti/correct/center_single/8558/occulstion_sensitivity/RAVEN_8558_test.npz_matrix.png\n",
      "Saved choices heatmap to: ruti/correct/center_single/8558/occulstion_sensitivity/RAVEN_8558_test.npz_choices.png\n",
      "Saved matrix heatmap to: ruti/correct/center_single/2529/occulstion_sensitivity/RAVEN_2529_test.npz_matrix.png\n",
      "Saved choices heatmap to: ruti/correct/center_single/2529/occulstion_sensitivity/RAVEN_2529_test.npz_choices.png\n",
      "Saved matrix heatmap to: ruti/wrong/center_single/8608/occulstion_sensitivity/RAVEN_8608_test.npz_matrix.png\n",
      "Saved choices heatmap to: ruti/wrong/center_single/8608/occulstion_sensitivity/RAVEN_8608_test.npz_choices.png\n",
      "Saved matrix heatmap to: ruti/correct/in_center_single_out_center_single/9138/occulstion_sensitivity/RAVEN_9138_test.npz_matrix.png\n",
      "Saved choices heatmap to: ruti/correct/in_center_single_out_center_single/9138/occulstion_sensitivity/RAVEN_9138_test.npz_choices.png\n",
      "Saved matrix heatmap to: ruti/correct/in_center_single_out_center_single/5878/occulstion_sensitivity/RAVEN_5878_test.npz_matrix.png\n",
      "Saved choices heatmap to: ruti/correct/in_center_single_out_center_single/5878/occulstion_sensitivity/RAVEN_5878_test.npz_choices.png\n",
      "Saved matrix heatmap to: ruti/wrong/in_center_single_out_center_single/7409/occulstion_sensitivity/RAVEN_7409_test.npz_matrix.png\n",
      "Saved choices heatmap to: ruti/wrong/in_center_single_out_center_single/7409/occulstion_sensitivity/RAVEN_7409_test.npz_choices.png\n",
      "Saved matrix heatmap to: ruti/wrong/in_center_single_out_center_single/7899/occulstion_sensitivity/RAVEN_7899_test.npz_matrix.png\n",
      "Saved choices heatmap to: ruti/wrong/in_center_single_out_center_single/7899/occulstion_sensitivity/RAVEN_7899_test.npz_choices.png\n",
      "Saved matrix heatmap to: ruti/correct/in_distribute_four_out_center_single/5878/occulstion_sensitivity/RAVEN_5878_test.npz_matrix.png\n",
      "Saved choices heatmap to: ruti/correct/in_distribute_four_out_center_single/5878/occulstion_sensitivity/RAVEN_5878_test.npz_choices.png\n",
      "Saved matrix heatmap to: ruti/correct/in_distribute_four_out_center_single/7789/occulstion_sensitivity/RAVEN_7789_test.npz_matrix.png\n",
      "Saved choices heatmap to: ruti/correct/in_distribute_four_out_center_single/7789/occulstion_sensitivity/RAVEN_7789_test.npz_choices.png\n",
      "Saved matrix heatmap to: ruti/correct/in_distribute_four_out_center_single/219/occulstion_sensitivity/RAVEN_219_test.npz_matrix.png\n",
      "Saved choices heatmap to: ruti/correct/in_distribute_four_out_center_single/219/occulstion_sensitivity/RAVEN_219_test.npz_choices.png\n",
      "Saved matrix heatmap to: ruti/wrong/in_distribute_four_out_center_single/2469/occulstion_sensitivity/RAVEN_2469_test.npz_matrix.png\n",
      "Saved choices heatmap to: ruti/wrong/in_distribute_four_out_center_single/2469/occulstion_sensitivity/RAVEN_2469_test.npz_choices.png\n",
      "Saved matrix heatmap to: ruti/correct/left_center_single_right_center_single/5348/occulstion_sensitivity/RAVEN_5348_test.npz_matrix.png\n",
      "Saved choices heatmap to: ruti/correct/left_center_single_right_center_single/5348/occulstion_sensitivity/RAVEN_5348_test.npz_choices.png\n",
      "Saved matrix heatmap to: ruti/correct/left_center_single_right_center_single/218/occulstion_sensitivity/RAVEN_218_test.npz_matrix.png\n",
      "Saved choices heatmap to: ruti/correct/left_center_single_right_center_single/218/occulstion_sensitivity/RAVEN_218_test.npz_choices.png\n",
      "Saved matrix heatmap to: ruti/wrong/left_center_single_right_center_single/7969/occulstion_sensitivity/RAVEN_7969_test.npz_matrix.png\n",
      "Saved choices heatmap to: ruti/wrong/left_center_single_right_center_single/7969/occulstion_sensitivity/RAVEN_7969_test.npz_choices.png\n",
      "Saved matrix heatmap to: ruti/correct/up_center_single_down_center_single/7029/occulstion_sensitivity/RAVEN_7029_test.npz_matrix.png\n",
      "Saved choices heatmap to: ruti/correct/up_center_single_down_center_single/7029/occulstion_sensitivity/RAVEN_7029_test.npz_choices.png\n",
      "Saved matrix heatmap to: ruti/correct/up_center_single_down_center_single/219/occulstion_sensitivity/RAVEN_219_test.npz_matrix.png\n",
      "Saved choices heatmap to: ruti/correct/up_center_single_down_center_single/219/occulstion_sensitivity/RAVEN_219_test.npz_choices.png\n",
      "Saved matrix heatmap to: ruti/wrong/up_center_single_down_center_single/8669/occulstion_sensitivity/RAVEN_8669_test.npz_matrix.png\n",
      "Saved choices heatmap to: ruti/wrong/up_center_single_down_center_single/8669/occulstion_sensitivity/RAVEN_8669_test.npz_choices.png\n"
     ]
    }
   ],
   "source": [
    "for category, results in data_dict.items():\n",
    "    for label in [\"correct\", \"wrong\"]:\n",
    "        for sample_id in results[label]:\n",
    "            filename = f\"RAVEN_{sample_id}_test.npz\"\n",
    "            path = os.path.join(base_dir, category, filename)\n",
    "            out_dir = os.path.join(save_root, label, category, str(sample_id), 'occulstion_sensitivity')\n",
    "\n",
    "            data = np.load(path)\n",
    "            images = data['image']  # (16, 160, 160)\n",
    "            images_resized = np.stack([cv2.resize(img, (96, 96), interpolation=cv2.INTER_NEAREST) for img in images])\n",
    "            \n",
    "            # Convert to 3-channel tensors\n",
    "            tf = ToTensor()\n",
    "            images_tensor = tf(images).unsqueeze(0).to(torch.device(\"mps\"))\n",
    "            images_tensor_resized = tf(images_resized).unsqueeze(0).to(torch.device(\"mps\"))\n",
    "            \n",
    "            # Predict\n",
    "            with torch.no_grad():\n",
    "                output = model(images_tensor)\n",
    "                output_resized = model(images_tensor_resized)\n",
    "                # pred = torch.argmax(output, dim=1).item()\n",
    "                pred = torch.argmax(output_resized, dim=1).item()\n",
    "            \n",
    "            try:\n",
    "                visualize_occlusion_with_heatmaps(\n",
    "                    model=model,\n",
    "                    images_tensor=images_tensor,\n",
    "                    original_images=images,\n",
    "                    pred=pred, \n",
    "                    save_dir=out_dir,       \n",
    "                    filename_prefix=filename \n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Failed on {path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bdeda8-8f63-4e84-8cf3-d80dd2ef21aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv-dcnet)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
